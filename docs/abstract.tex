\section*{Zusammenfassung}

Diese Arbeit wird sich mit verschiedenen 
Optimierungsalgorithmen des Gradienten Abstiegsverfahren bei neuronalen Netzen beschäftigen
und diese auf Basis von Beispieldatensätzen evaluieren.
Hierbei wird eine Metrik definiert um die Ergebnis der einzelnen 
Optimierungsmethode zu vergleichen. 

Ein Optimierungsalgorithmus ist eine Möglichkeit
die Konvergenz der Fehlerfunktion $J(\theta)$ des neuronalen Netzes
beim Lernen zu verbessern. 

Hierbei wird auf den Lern Prozess des Neuronalen
Netzes eingegangen. Besonderen Fokus wird der ``Gradient Descent'',
zu Deutsch Gradienten Abstiegsverfahren, einnehmen, da dies die Grundlage
des Lernens darstellt. Dieser sucht im mehrdimensionalen Raum
die Minima der nichtlinearen Fehlerfunktion. Die Verbesserung 
dieser Suche machen sich die Optimierungsalgorithmen zur Aufgabe.  
Nach der theoretischen Aufarbeitung,
werden wir ein paar Eigenschaften über
diese Optimierungsmethoden annehmen
,um diese anhand der Testdaten zu überprüfen und mit der
Literatur zu vergleichen. 


\section*{Abstract}

This work will focus on explaning the different optimization methods of the gradient descent
algorithm of neural networks and evaluating them
on example datasets.
We will define a metric to be able to evaluate the performance 
of the different optimizer.

An optimization method is a way to improve the performance of the error 
function $J(\theta)$ of the neural network. 


Furthermore this work will give a detailed explanation of the learning process of Neural Networks
especially focusing on the Gradient Descent, which is the foundation of learning in neural networks.
This algorithm aims to find local minima in the Hyperplane of the non-linear error function. 
The optimization algorithms aim to improve this search. 
After the Theory, we will assume some properties about those optimization methods and test those assumptions
by evaluating the metrics of the neural networks. Then we will compare our results to
the literature
